{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный Байесовский классификатор\n",
    "\n",
    "Самый простой, часто использующийся и при этом один из самых элегантных и эффективных алгоритмов классификации — Наивный Байесовский классификатор (НБК). Он:\n",
    "\n",
    "* Устойчив к незначимым признакам, так как просто игнорирует их\n",
    "* Быстро обучается и быстро возвращает предсказание\n",
    "* Потребляет относительно небольшое число ресурсов\n",
    "* Показывает один из лучших результатов в задаче анализа тональности текста\n",
    "\n",
    "Наивный Байесовский классификатор основан на теореме Байеса о вероятности события. Наивность его заключается в предположении, что все анализируемые признаки независимы, это помогает упростить работу классификатора. На практике чаще всего бывает наоборот — все признаки так или иначе влияют друг на друга. Однако \"наивный\" алгоритм всё равно показывает хорошие результаты, и это главное.\n",
    "\n",
    "Ниже мы подробнее изучим основу алгоритма - саму теорему Байеса и предположение, означающее \"наивность\" классификатора, а затем рассмотрим его результаты на конкретном примере с классификацией твитов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теорема Байеса\n",
    "\n",
    "Суть Наивной Байесовской классификации — отслеживание того, какой признак о каком классе свидетельствует. А то, какие признаки и каким образом рассматриваются, зависит от модели обучения классификатора. \n",
    "\n",
    "Например, в **модели Бернулли** допускаются только булевские признаки (то есть признаки типа есть/нет). При анализе текстов эта модель будет анализировать, встречается определённое слово в тексте или нет, но не будет обращать внимание на то, **сколько раз** оно встречается. А в **мультиномиальной модели** признаками являются счетчики слов, то есть модель будет не только замечать наличие слова в тексте, но и считать, как много раз оно было использовано.\n",
    "\n",
    "Для объяснения работы НБК при анализе тональности текста рассмотрим _ _модель Бернулли_ _, как более простую. Позже, когда мы перейдём к реальным классификаторам и задачам, будем использовать _ _мультиномиальную модель_ _.\n",
    "\n",
    "Итак, пусть:\n",
    "* Класс твита по тональности (положительный или отрицательный);\n",
    "* Признак 1: в твите хотя бы раз встречается слово good;\n",
    "* Признак 2: в твите хоть раз встречается слово bad.\n",
    "\n",
    "В процессе обучения на уже размеченном по тональности корпусе твитов алгоритм строит наивную Байесовскую модель, которая для каждого твита возвращает вероятность его принадлежности к определённому классу, если известны значения обоих признаков.\n",
    "\n",
    "Так как мы не можем посмотреть внутрь модели и посчитать вероятность принадлежности твита к определённому классу напрямую, то по формуле Байеса мы можем высчитать её через вероятность класса самого по себе и вероятность наличия в твите слов-признаков.\n",
    "\n",
    "В виде формулы это выглядит примерно как:\n",
    "\n",
    "$$probability = \\frac{class \\times likelihood}{features}$$\n",
    "\n",
    "$class$ и $features$ легко посчитать:\n",
    "* $class$ - вероятность класса без каких-либо знаний о данных. Оценить её можно напрямую подсчитав долю обучающих примеров, принадлежащих данному классу.\n",
    "* $features$ - вероятность одновременного наличия слов-признаков в проверяемом твите.\n",
    "\n",
    "Проблемы возникают при вычислении likelihood - вероятности наличия в твите определённого класса обоих слов-признаков. Но здесь в дело вступает второй важный компонент Наивного Байесовского классификатора - его \"наивность\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предположение о наивности\n",
    "\n",
    "Поиск вероятности одновременного наличия нескольких признаков в тексте, принадлежащем определённому классу - задача достаточно сложная, так как признаки становятся зависимы друг от друга, а \"посмотреть\" на их зависимость мы не можем. Мы даже не знаем, как эта зависимость выражается и есть ли она вообще.\n",
    "\n",
    "Здесь и кроется выход: мы можем \"наивно\" предположить, что все признаки независимы друг от друга. Тогда для вычисления нужной вероятности надо всего лишь высчитать вероятность наличия каждого слова по отдельности в тексте определённого класса. Это сделать гораздо проще.\n",
    "\n",
    "Собирая всё вместе, для примера с двумя словами-признаками получаем простую формулу:\n",
    "\n",
    "$$probability = \\frac{class \\times good(class) \\times bad(class)}{features}$$\n",
    "\n",
    "где:\n",
    "\n",
    "* good(class) - вероятность наличия слова-признака good в твите выбранного класса \n",
    "* bad(class) - вероятность наличия слова-признака bad в твите выбранного класса \n",
    "\n",
    "Любопытная вещь №1: хотя теоретически неправильно выдвигать произвольные предположения под настроение, в данном случае такой подход на удивление хорошо работает в реальных задачах.\n",
    "\n",
    "Любопытная вещь №2: для любого количества классов знаменатель дроби - features - всегда будет одинаковым, а так как при предсказании выбирается наибольшая вероятность, то его можно проигнорировать. Наибольшее значение от этого не изменится, а формула станет проще.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование наивного Байесовского алгоритма для классификации\n",
    "\n",
    "После того, как модель обучилась на размеченнном корпусе, она может предсказывать тональность новых твитов, которых не было в корпусе для обучения.\n",
    "\n",
    "Получив новый твит, она вычисляет вероятности его принадлежности к каждому из двух классов по формуле выше, а затем относит твит к тому классу, для которого вероятность была больше. Стоит отметить, что сами вероятности классификатор не сильно интересуют: гораздо важнее то, какой класс более правдоподобен для того или иного текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проиллюстрируем, чтобы понаблюдать за работой наивного байесовского алгоритма. Сделаем предположение, что Twitter разрешает употреблять только два слова: good и bad и что мы уже вручную проклассифицировали несколько твитов:\n",
    "\n",
    "| Твит | Класс \n",
    "| :-:  | :-:\n",
    "|good | pos\n",
    "|good | pos\n",
    "|good bad | pos\n",
    "|bad | pos\n",
    "|bad | neg\n",
    "|bad | neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Твит bad получил как отрицательную, так и положительную оценку (это возможно благодаря многозначности слов и контексту).\n",
    "Всего шесть твитов - 4 pos и 2 neg.\n",
    "\n",
    "Считаем вероятности классов:\n",
    "$$\\Pr(Class=pos) = \\frac{4}{6} ~ 0.67$$\n",
    "$$\\Pr(Class=neg) = \\frac{2}{6} ~ 0.33$$\n",
    "\n",
    "Это означает, что ничего не зная о самом твите, можно предположить, что он положительный. Просто их больше, но об этом мы будем говорить позже.\n",
    "\n",
    "Пока отсутсвует вычисление вероятностей слов-признаков при условии класса. Они вычисляются как количество твитов класса, в которых встречался отдельный признак, поделенное на количество твитов, помеченных этим классом.\n",
    "\n",
    "Вероятность встретить good, если известно что класс положительный:\n",
    "\n",
    "$$\\Pr(good \\mid Class=pos) = \\frac{\\text{число положительных твитов, содержащих слово good}}{\\text{число всех положительных твитов}} = \\frac{3}{4}$$\n",
    "\n",
    "Поскольку из 4 положительных твитов 3 содержали слово good.\n",
    "\n",
    "Очевидно, что вероятность не встретить слово good в положительном твите равна:\n",
    "\n",
    "$$1 - \\Pr(good \\mid Class=pos) = 0.25$$\n",
    "\n",
    "Точно так же производятся остальные вычисления. \n",
    "\n",
    "Итоговая вероятность класса какого-либо нового твита будет зависеть от того, есть ли в нём слова good и bad. Мы умножаем вероятность класса на вероятности наличия (или отсутствия) в нём слов-признаков. Все эти компоненты уже посчитаны, и вычислить итоговые вероятности не составит труда.\n",
    "\n",
    "Но как быть со словами не встречавшихся в тренировочном корпусе? Ведь всем новым словам будет присвоена нулевая вероятность..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Учет ранее не встречавшихся слов\n",
    "\n",
    "Как мы уже говорили, модель вычисляет не точные вероятности, а только правдоподобия для определения лучшего класса. Однако, используя обучающий корпус, мы предполагаем, что данные корпуса содержат полную информацию обо всех вероятностях, просто мы не используем их.\n",
    "\n",
    "Но разве какой угодно большой корпус твитов может содержать информацию обо всех твитах, которые когда-то написаны и будут написаны? Разумеется, нет.\n",
    "\n",
    "Если же сузить до нашего примера с шестью твитами, эту модель может ввести в ступор появление третьего слова - например, слова film. Стоит появиться таким новым твитам, и сразу станет понятно, что приблизительные вычисления нашей модели совсем не соответствуют новой реальности.\n",
    "\n",
    "Однако учесть все слова невозможно, и эту проблему приходится обходить. Самый простой способ её решения - **сглаживание с прибавлением единицы (add-one smoothing)**.\n",
    "Это очень простой приём, заключающийся в прибавлении единицы ко всем вхождениям всех признаков. В его основе лежит разумное предположение, что даже если мы не видели данного слова в обучающем корпусе, есть шанс , что это случилось только потому, что в нашей выборке таких твитов не оказалось. Однако для баланса единица прибавляется и к количеству новых признаков (чтобы избежать нулей), и к количеству старых (которое просто увеличивается на 1).\n",
    "\n",
    "То есть вместо вычисления:\n",
    "\n",
    "$$\\Pr(good \\mid Class=pos) = \\frac{\\text{число положительных твитов, содержащих слово good}}{\\text{число всех положительных твитов}} = \\frac{3}{4}$$\n",
    "\n",
    "Мы вычисляем:\n",
    "\n",
    "$$\\Pr(good \\mid Class=pos) = \\frac{3+1}{4+2}=0.67$$\n",
    "\n",
    "Почему в знаменателе прибавлено 2? \n",
    "Потому что всего у нас два признака: вхождения слов good и bad. Поскольку мы прибавляем 1 для каждого признака, нужно позаботиться, чтобы получились всё те же вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как построить наивный Байесовский классификатор в Python?\n",
    "\n",
    "Наивный Байесовский классификатор уже реализован за нас в библиотеке Scikit-learn (или sklearn). Всё, что требуется - дать модели обучающие данные и данные для предсказания.\n",
    "\n",
    "Scikit-learn предлагает 3 модели Наивного Байесовского классификатора: \n",
    "\n",
    "* Gaussian: предполагает, что атрибуты нормально распределены.\n",
    "\n",
    "* Multinomial: используется для дискретных атрибутов. Например, при текстовой классификации она будет считать, сколько раз слово-признак встречается в анализируемом тексте.\n",
    "\n",
    "* Bernoulli: модель Бернулли, оперирует бинарными параметрами типа \"0 или 1\". Наример, классификация текстов с моделью bag of words, где атрибуты могут быть 0 (слово не встретилось) или 1 (слово встретилось).\n",
    "\n",
    "Ниже рассмотрен пример использования Гауссовской модели наивного Байесовского классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "#импортируем модель из библиотеки sklearn и библиотеку numpy для работы с массивами\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "#зададим матрицу признаков обучающих объектов и вектор значений классов, к которым они относятся\n",
    "x= np.array([[-3,7],[1,5], [1,2], [-2,0], [2,3], [-4,0], [-1,1], [1,1], [-2,2], [2,7], [-4,1], [-2,7]])\n",
    "Y = np.array([3, 3, 3, 3, 4, 3, 3, 4, 3, 4, 4, 4])\n",
    "#создаём модель\n",
    "model = GaussianNB()\n",
    "\n",
    "# обучаем модель на заданных данных\n",
    "model.fit(x, Y)\n",
    "\n",
    "#дадим модели признаки ещё двух объектов и получим предсказанные им классы\n",
    "predicted= model.predict([[1,2],[3,4]])\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
